
    kiterunner_t
    TO THE HAPPY FEW


Linux内核为上层应用提供了一个封装。在写应用代码、系统性能调优等工作时，只有了解底层的实现，才知道更好的选择是怎样的。本文根据内核2.6.32（其中信号是参考的2.6.24版本的资料）进行概述，大致了解底层做了些什么事，是怎样做的。主要描述进程、内存、时间、中断、信号、文件系统、IO、sysfs，本文不涉及到网络协议栈、线程、事件机制（epoll）、procfs等，内存只简单介绍内存管理用到哪些算法和地址转换过程，具体算法实现原理不涉及。

内核栈
中断栈
信号栈

并发的来源
SMP
中断
抢占式调度
内核抢占

## 1 进程

## 2 中断

## 3 信号


## 4 内存

## 5 时间


## 6 文件系统
### 6.1 VFS
VFS是一个代理，是文件系统的一个通用模型，它概括了文件系统常见的功能和行为，使得上层应用可以不关心底层的实现，可以在系统中并存多种文件系统。内核中VFS通过OO的方式提供了主要的四个对象及其操作，如下

![kernel-vfs-table-datastruct][1]


FS主要关注从不同的视角去了解文件系统的功能和实现方案（画出结构示意图，需要更进一步的了解，目前只所知甚少）：

* 硬盘上的实际文件组织方式。这与实际文件系统息息相关，如ext3/ext4. 
* 内核组织所有文件系统的方式，如何对每个进程呈现一个统一的文件树结构。主要用到的数据结构是通过file_system_type、vfsmount来完成。
* 进程看到的文件系统。在进程结构中通过file_struct（进程已打开的文件），fs_struct（进程中关于文件系统的一些信息，如当前目录、根目录等信息），mnt_namespace这些对象来展现。

VFS是一个前端，通过一个统一的抽象模型，很好的给系统了一个统一的视角去表达对各种文件的表达要求。系统通过简单的文件描述符，统一了各种资源（如文件、信号、定时器、内核对象、事件等）的表达，使得epoll能在应用层展现更强大的能力。

## 7 IO
看似普通的IO操作，在内核层却要经历很多。从应用层发出IO调用，如下图所示，首先经历VFS处理，在处理过程中根据不同的IO要求，需要进行页缓存，再提交到块IO层，经IO调度程序调度后送到设备驱动层进行真正IO操作。该小节包括VFS的基本对象、不同视角了解文件系统的关注点（还未理解到位），重点放在对文件系统的IO操作时内核各个部分是如何配合完成工作的。

![kernel-io][2]


### 7.1 页缓存
从上面我们知道大部分IO并非直接提交给块IO模块进行调度，而是需要经过也高速缓存模块进行缓存。缓存的理由就用不着在这里抄书了。缓存可以将其生命周期描述为创建、同步、回收，而内核也围绕这三个部分作出了不同的选择。曾经，内核将缓存分为页缓存和缓冲区缓存（或者说块缓存），而鉴于内核对内存的基本管理对象是页，在2.6.32中，二者就融合为一页缓存。

创建：页缓存是通过对文件的读把相应的内容装载进内存并进行缓存而创建的。读操作时，首先在缓存中查找数据是否已经被缓存了，若没有，则将数据载入，然后通过缓存返回给上层应用，从而创建了相应的缓存对象。

同步：同步主要针对写操作。当我们需要对文件进行写操作时，对于缓存中的数据和实际文件的数据的不一致有3种策略：

* 不缓存：写时，跳过缓存，直接将数据写到磁盘，同时使缓存数据失效。
* 通写：写时，既更新缓存，又更新文件。
* 回写：写时，只将数据更新到缓存，同时将该缓存数据标识为脏页，添加到脏页链表，由后台内核线程周期性更新到文件。（内核默认采用回写）

回收：回收按照以下步骤进行，(1) 替换干净页；(2) 刷新脏页，腾出内存空间，使用的策略有：预测算法，LRU，双链策略LRU/n（实际中通常有多个链表，n代表链表数）。使用LRU的最大问题是只访问一次的页在内存中形成浪费，因此改进策略是LRU/n。对于双链来说，内核将链表划分为活跃链表和非活跃链表，第一次访问时加入非活跃链表中，再次访问时移动到活跃链表中。


* 页中缓存的磁盘块对象并不一定连续，如何在缓存中查找对应的块是否在存在？
* 页缓存是通用的，需要能缓存所有的基于页的对象，包括普通文件、块设备文件、内存映射文件。
基于以上两个原因，内核使用address_space管理缓存项和页IO操作。

address_space与vm_area_struct结构体的关系？对于普通文件的读写来说，进程地址空间中并没有单独的vm_area_struct结构体与之对应，那么是否是在诸如堆内存中进行分配页，然后对应到对应的vm_area_struct中？address_space是否对于每个文件来说都会有一个呢？

读写普通文件时，在内存中存在两份拷贝，如下图所示，一是内核页缓存page，一份是用户堆栈区的缓冲区page。

![kernel-io-regular-file][3]


内存文件映射时，在内存中只存在一份拷贝。

匿名映射

vm_area_struct描述了内存进程地址空间中的一个区域。

address_space_operations实现页IO操作，每个后端存储都通过该结构体描述如何与页缓存交互。

内核缓存的对象是页，而页主要来自普通文件、块设备文件和内存映射文件的读写。

由于页缓存的作用，写操作会被延迟。内核线程会在以下情况发生时执行写回动作：

* 空闲内存低于阀值。内核必须将脏页写回磁盘以释放内存。
* 脏页在内存中停留时间超过阀值。
* 用户进程调用sync()或fsync()。
/proc/sys/vm下的内核参数变量可以控制页缓存的这些行为，见下表。

![kernel-io-table-flusher-param][4]


在2.6内核以前，bdflush和kupdated两个线程完成上述三个任务。在2.6以后，使用pdflush线程，系统中有固定数目的该线程（2-8个，可配置）。较新内核（2.6.32以后）中，flusher线程承担了这三个线程的角色，而且每个磁盘都有一个flusher线程，提高了磁盘的吞吐率。

### 7.2 块IO
当来自应用层的请求经过VFS/缓存最后到达块IO时，块IO模块所做的就主要是两件事：一是将所有请求组织成链表，二是对所有请求进行合理的调度。

磁盘操作的最小单位是块，大多数系统中，一个磁盘块的大小是512字节，而内核却使用页来管理内存，一个也通常是4KB或8KB。内核使用buffer_head来表示二者之间的映射关系。内核为了更高效的IO操作，需要尽量缩短磁盘寻址时间，内核就利用排序合并、调度来尽量保证磁盘的寻址时间更短。

内核用队列来管理所有IO请求，而并不是直接将每个IO提交给磁盘。如下图所示，请求由request_queue和quest结构组织成链表；每个请求则由bio来表示。每次IO请求提交给该队列时，内核会根据不同的调度算法进行相应的合并和排序操作。合并是将两个或多个相邻的请求合并成一个，尽量减少请求的次数和磁盘寻址次数。新请求正好连在一个现有请求前，则执行向前合并；反之，新请求在现有请求之后，则执行向后合并。不过，一般文件都是以扇区号增长的方式进行存储的，IO操作也是从头读到尾的，较少反向读，因此向后合并的情况较多。排序是指整个请求队列按扇区增长方向有序排列，通过保持磁头以直线方向移动，缩短了所有请求的磁盘寻址时间。由于这种调度与电梯类似，因此IO调度器也被成为电梯调度。
内核有若干不同的IO调度器算法实现，内核启动命令行选项elevator可以对齐进行设置。

![kernel-io-table-scheduler][5]


在2.4内核中，默认的调度算法是Linus电梯。在新请求加入Linus电梯队列时，可能发生四种操作，依次是：

* 队列中存在相邻磁盘扇区的请求，则新请求与之合并；
* 队列中存在驻留时间过长的请求，则新请求加入到队列末尾，防止旧请求饥饿；
* 队列中存在以扇区增长方向有序的合适插入位置，则新请求插入该位置；
* 不存在合适的插入位置，则新请求被插入到队列尾部。
Linus电梯的最大问题在于如果对于相邻磁盘位置请求太多，则容易造成其他磁盘位置的请求饥饿。步骤2中的年龄检测机制只是改善了情况，并不能遏制。

针对Linus电梯调度的问题，人们提出了最终期限调度。排序队列按照Linus电梯策略进行调度，同时根据请求类型将不同请求按请求时间分别插入读、写队列，每个请求设置超时时间，读为500ms，写为5s。IO调度时，排序队列的仍然按照Linus电梯策略进入派发队列（同时会删除对应的读、写队列的请求），若读写请求头有超时的，则将对应请求插入派发队列。这样就避免了饥饿现象。但该算法并不严格保证请求的响应时间。

![kernel-io-deadline][6]


预测调度基于最终期限调度，改进点在于增加了预测启发能力。在提交请求（@WHY我想应该是将请求提交到派发队列时）后，并不直接返回处理其他后续排序请求，而是等待片刻（默认6ms，改时间可配置），如果在等待期间有对相邻磁盘位置的IO请求，则将其进行提交。这样在预测率高时减少了磁盘寻址时间。这是系统默认的IO调度器。

完全公平调度为每个进程分配一个队列。调度器以时间片轮转，从每个队列中选取一定的请求（默认为4）进行调度。该算法保证了进程级的公平性，适用于桌面应用，但在其他工作负载中也有不错的工作性能。

空操作调度将所有请求完全按照请求先后顺序放入请求队列，并不进行排序，只将相邻请求进行合并。该调度策略适用于flash类纯随机访问设备。

## 8 设备管理
sysfs

## 9 参考资料

* Linux内核设计与实现，第3版。
* 独辟蹊径品内核——Linux内核源代码导读，李云华著。
* [http://duartes.org/gustavo/blog/post/page-cache-the-affair-between-memory-and-files/][7]。
* [http://www.penglixun.com/tech/system/linux_cache_discovery.html][8]。


[1]: images/kernel/kernel-vfs-table-datastruct.png "kernel-vfs-table-datastruct"
[2]: images/kernel/kernel-io.png "kernel-io"
[3]: images/kernel/kernel-io-regular-file.png "kernel-io-regular-file"
[4]: images/kernel/kernel-io-table-flusher-param.png "kernel-io-table-flusher-param"
[5]: images/kernel/kernel-io-table-scheduler.png "kernel-io-table-scheduler"
[6]: images/kernel/kernel-io-deadline.png "kernel-io-deadline"
[7]: http://duartes.org/gustavo/blog/post/page-cache-the-affair-between-memory-and-files/
[8]: http://www.penglixun.com/tech/system/linux_cache_discovery.html
